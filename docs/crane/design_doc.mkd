Design Documentation and Vision
================================

## Introduction

This document describes the design of the annotated data extraction (and loading) system.

It describes the basic ETL model and provides detailed documentation
for all operations, including input/output data formats.

The design for the implementation details **ARE NOT DOCUMENTED**

For additional learning material and references, see the [education document](../education.mkd)

All testing, quality, construction, and risk assessment procedures are in the [construction document](../construction.mkd)


### Overview of existing ETL services/tools used by CR

 * TODO


### Commonly used terms

 * **JSON** - JavaScript Object Notation; a data serialization notation
 * **edn** - Also EDN; [Extensible Data Notation](https://github.com/edn-format/edn); a data serialization notation, like JSON
 * TODO


### Goals

The goal of this document is to elaborate on the constraints, trade-offs, formats,
and general architecture of the system.  These decisions ensure the system can
easily evolve and adapt, while achieving the target quality attributes and
auxiliary goals.

**This is a living document, it should evolve along with the system.**


# Consumer Reports: Annotated Extractor

The **Consumer Reports: Annotated Extractor** is a general purpose tool for
loading existing CR data sets into Datomic.  The main tool
(`cr-ane`), accompanies the larger `cr-ocs` system, to allow bootstrapping of
data while prototyping new API functionality.  While small in scope, `cr-ane`
unlocks unrealized potential for the `cr-ocs` system.

The two operates in two phases:

 1. Parse a data source and extract hints about the schema
   * This is used to provide data modeling input when creating the schema by hand
   * The Schema is defined within a CR-ocs Descriptor file
 2. Given a schema, parse and import a data source into Datomic

The **architecture diagram** below serves as an example upon which to base a mental model.

TODO Diagram

The **sequence diagram** below shows the simplest usage of the container service.

TODO Diagram


## Data types and coercion

During the ingestion phase, data must be coerced from source specific formats
(JSON/XML/edn) into a data type acceptable from Datomic. Some types such as
boolean or string values, will not require coercion at all. Other types may
have source-dependent encodings, like dates that are stored as doubles.  These
entries will need to be stored as a date-type, and so quite a bit of coercion
will be needed.

This coercion will be accomplished via schema reflection. After the Schema
extraction phase the user of the import program will be required to define a
mapping of JSON attributes to Datomic attributes. The coercion code will then
use those mappings to load the schema definition from Datomic that corresponds
to a given JSON attribute. Reflecting on the input type will then give the
process a input Java type to output Java type pair. This pair will then be fed
into a polymorphic dispatch function that will run the actual conversion.

Some coercions (to refs for example) may need to create additional Datomic
entities. Therefore the coercion functions will need the ability affect the
entire output transaction, not just the ability to change the output value of a
given attribute.


## Extensible ingestion sources

Incoming data destined for import takes on many shapes and formats.  Product
data and reviews are in hierarchical JSON, taxonomy related data is in specific XML documents,
and other information may come from additional sources like SQL or CSV dumps.
To enable the tool to be modifiable and adapt to new environments with various
sources of data, the initial data extraction must be open for extension.

File type extensions, content type, and file preambles are all possible dispatch points for
the initial ingestion of various data sources.  To start, file type extensions
will be used, but other approaches will be added as needed.

The initial ingestion will produce a normalized format of the source data,
represented as edn.  The normalized form will not attempt to change the internal
structure of the data, but will apply some amount of shaping to enable the
ETL pipeline to make some assumptions about how to parse the resulting data.

The normalized format includes the file, file extension, file path parts, and
an approximate guess at the hierarchy.  As the normalized payloads are processed
a `data` attribute extracts a hash-map representation of the data found in the
in the file.  For certain filetypes, like XML, some data is discarded to make
the map more attribute-value centric.


## Schema extraction

Before data can be imported, a schema must exist. This schema can be partially
inferred from the input sources. The schema extraction phase exists purely to
generate a list of known attributes, possible data types, and their
cardinality. This will be accomplished via walking the entire input tree and
creating a list attributes found in the JSON data. After executing the
completed list will compare the suggested attributes to existing attributes
found in Datomic. Only the non-existing attributes will be reported.

A example may be the best way show how this would work. Feeding the following
JSON data into the Schema Extraction process:

```json

{"productName" : "10 Speed Super Bike",
 "productID" : 44232112,
 "isActive" : true
 "vendors" :  [{"vendorName" : "Amazon",
                "price" : 44.22,
                "asOfDate" : 11234232.11234}
               {"vendorName" : "Walmart",
                "price" : 44,
                "asOfDate" : 11234422.11212}]}
```

would result in the following output from that process (assuming an empty database):

```

Suggested Attributes:

productName -> string
productID -> long
isActive -> boolean
vendors -> many ref
vendors/vendorName -> string
vendors/price -> double, long
vendors/asOfDate -> double

```

Vector values signal that the type they contain will be of `many` cardinality.
Thus `[1 2 3]` would be interpreted as `many long` while `[ {} {} ]` would be
interpreted as `many ref`

Sub entities will have the name of their parent attributes prefixed to the
name. In the above example the vendorName attribute is contained in an entity
referenced by a vendor attribute in a different entity. Therefore the suggested
name is `vendors/

Notice how `asOfDate` is inferred to be a double even though it is actually a
date, this will be up to the actual schema designer to translate to the proper
type. Also, `price` has two types as both appear in the input data. Once again
it'll be be up to the implementor to decide on one or the other.

This output data should also be used to create name mappings between JSON keys
and Datomic attributes. See Data coercion for more information on how this is
used.

A more advanced schema hinter should additionally be able to ignore certain
attributes, transform certain attributes, and apply transformations/override to
specific reported datatypes.  The advanced hinter should also be open for
extension to handle/normalize new datatypes produced during the source ingest.
Using these advanced features should only be optional, and the default should
be using a simpler hinter.


## Annotated extraction and loading

By this point of the import process, a schema will exist within a CR-ocs
Descriptor file and mappings will have been created. The next step is to import
the actual data. This process will use much of the same machinery used by the
schema extraction. However, in this case the actual data will also be loaded.
Each JSON file will be loaded, converted to datoms and each value will be
coerced to a Datomic type. Any nested hash-maps will become ref attributes to
other entities. Therefore a single JSON file could contain multiple Datomic
entities. It is highly likely that there will be 1:1 ratio of input files to
Datomic transactions. Although there is probably no technical reason for this.

A unique key will be defined for each top level entity (such as Product), this
will allow future import processes to append to existing entities. This will
allow data such as User Reviews to be attached to existing entities, even
though the input data exists in separate files.


## Data integrity and compatibility

Some attributes may represent foreign keys to data provided outside of the
current import or JSON file. These links cannot be inferred from JSON and
instead must be linked up in a separate step. Therefore the final pass will
create these links. For example, if a `vendor` has a `vendorID`. That may be a
string on initial import. The Data integrity step will then use a `vendorRef`
attribute to directly link to the vendor entity once it has been imported. This
should make exploration of the imported data much more natural.

