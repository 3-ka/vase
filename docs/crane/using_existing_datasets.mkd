
Using existing data with CR-ocs
===============================

This guide describes how to import existing datasets into a CR-ocs service/Datomic.
It assumes you understand how CR-ocs works, and have [built your own API](../your_first_api.mkd)

The beginning of this guide will target importing product and review data
dumps, as they exist today in Consumer Reports.  The second half of the guide
will explain how to extend **CR-ane**, the data loading tool, to handle other
datasets.  Along the way, we'll establish some data modeling heuristics when
using Datomic.  Please read through the [CR-ane design doc](./design_doc.mkd)
for general information.

As you're following along, please **do not add and commit your data sets under/in Git**

The datasets are very large and can be unique to the person using CR-ocs.
Ensuring the data isn't committed avoids hampering the use of CR-ocs/CR-ane
for your fellow developers.


## Quick-start

CR-ane comes with support for importing/upserting product data from a JCR dump
(JSON files), and PWR review data (a single summary XML file).  This quick
start guide will cover both of those datasets.  If you need another dataset,
please skip this section and continuing reading the rest of the guide.

1. Clone a copy of the CR-ocs system if you haven't done so already

 ```bash
 git clone git@github.com:relevance/cr-ocs.git
 ```

2. Create a new directory at the root of the CR-ocs project to hold the data dumps.
   By convention, we name this directory `data`.

 ```bash
 cd cr-ocs
 mkdir data
 ```

3. Inside of the data directory, download and extract the JCR product data.
   You should end up with taxonomy directory at: `cr-ocs/data/jcr_root/data/productsAndServices/taxonomy`

 ```bash
 cd data
 # DOWNLOAD AND EXTRACT THE DATA
 ```

4. Download and extract the PWR review data inside the `data` directory.
   You should end up with review summary file at: `cr-ocs/data/pwr/93s485fs/rawdata/review_data_summary.xml`
5. From the data directory, change directories back to the project root: `cd ..`
6. Update your `:crane/path` attribute in your production system config file (`config/prod_system.edn`)
   to be the full path to JCR taxonomy directory. Save and exit.
7. Start a production CR-ocs: `lein run-prod` - let if fully start up
8. In another terminal window, start the importer: `lein run-import` (ignore all warnings/errors)
  * If you see an error at the end of import, just hit Ctrl-C; Errors happen because of stale threads
9. When the import is complete, update your production config file for the review walker and review data path.
   For example:

 ```clojure
 ...
 :crane/walker-for :reviews
 ...
 :crane/path "/Users/paul/code/cognitect/cr-ocs/data/pwr/93s485fs/rawdata/review_data_summary.xml"
 ```

10. Save the config file, and run the importer again: `lein run-import` (ignore all warnings/errors)
  * If import hangs after saying `Done.`, just hit Ctrl-C
11. You're all done.  You now have all the product and review data!

- - - - -

# Extending CR-ane to handle additional datasets

All of CR-ane's core components are open for extension - allowing developers to
enhance and improve its capabilities when handling additional datasets.

These components are:

 * The walker/normalizer - used to traverse a dataset (single or multi-file),
   and produce a lazy sequence of normalized *path-info maps* - individual
   sections of data.
   Default dispatch happens by filetype/file extension.
 * The hinter - consumes a sequence of path-info maps and produces a guess at
   the potential Datomic Schema
 * The importer - consumes a sequence of path-info maps and imports the data
   into Datomic, coercing values based on Datomic schema reflection and a coercion
   multimethod.

The following sections will explain each of the components in detail.
In general, these tools are used as follows:

 1. Using the walker/normalizer and one of the hinters, hint a schema for a given dataset
   * The extensible hinter (`:ext`), produces hints that are valid `short-schema-tx` vectors.
 2. Copy and enhance the schema in your descriptor file.
   * Mark identity, uniqueness, etc. and document the attributes
 3. Test out the importer to see if it'll work with: `lein run-import-test`
   * This runs an import against an in-memory datomic instance, and doesn't require a CR-ocs service to be running.
 4. If the test import works, import data into a running production CR-ocs: `lein run-import`

**A quick note**

Out of the box, CR-ane is capable of handling data from JSON, edn, and XML files.
Some datasets may be able to be imported using CR-ane without any code changes,
but more often than not, you'll need to update the code to shape/transform
the data.  Shaping the data happens at the "normalization" phase - all the phases
that follow (hinting and import), use the normalized data stream as input.


## Traversing the data

### Looking through your data

### The extensible normalizer

TODO: Describe filetype dispatch


## Hinting a schema

### Simple hinter

### Top hinter

### Extensible hinter

TODO: Make sure you recommend this one
TODO: Mention the substitution operations and `:crane_substituitions`


### Interpreting the results


## Data modeling best practices

Before you begin, you should fundamentally understand Datomic, its trade-offs,
and the potential it unlocks.  To help you, there's a [Datomic section](../education.mkd#datomic)
in the education material.  The best advice is to fully understand the
[information model](http://www.infoq.com/articles/Datomic-Information-Model).

What follows are some general heuristics you can adapt and apply.
These are purely Paul's opinions and do not reflect all of the wisdom that
exists on the Datomic product team.

#### Data is naturally relational

Data is naturally cohesive - sets of attributes group together to describe
*something*.  You should embrace that as you model data for Datomic, and
group related attributes under common namespaces.  It's usually best to begin
modeling data in a relational manner when you're starting with Datomic, or
starting with a new dataset (unless the data is inherently not relational).

#### Be agile

You'll occasionally realize a given attribute is cross-cutting and applies and
relates to many entities in your system.  In a traditional RDBMS, you might use
a series of link/join tables to achieve this affect.  In Datomic, you simply
share the common attribute with all the entities.

Closely related, you never need a "join table" to jump backwards to a relationship
in Datomic.  All attributes include and automatic back reference.
TODO: Put an example here.

Your schema isn't rigid and doesn't imply a shape, it's only a set of contracts
on attributes.  You can use those attributes however you want.  As we discussed
earlier, it makes sense to treat groups of these attributes like a cohesive set,
but if a part of your data feels more graph like, you can easily treat facts
like attributes about the graph relationships.

This is a fundamental shift from any other database technology - historically,
the database technology dictated how you modelled your data (A Graph DB, A Relational DB, etc),
with Datomic, you have *any* DB.  Your database is a collection of facts, and
those facts and be modelled and related however your application requires.
You can evolve the schema moving forward, as your application and its facts
evolve.

#### Leave hard-learned optimizations at the door

In traditional RDBMS systems, there is a spectrum of data shaping rules that
one applies to ensure acceptable performance. These do not apply to Datomic, and can be difficult for some to unlearn.

The best approach for modeling Datomic is always the simplest, most direct approach:
*What is the single attribute I really am trying to describe?*

As you build up from these low-level/direct attributes, you'll naturally compose
them using relationships.  In Datomic, those are `refs` or references to other
entities.

#### Play to Datomic's strengths

Datomic let's you come at data from any direction - entity-oriented (like
a relational store), attribute-oriented (like a graph DB), or any of the other
indexes.  Facts all have a notion of time, so accounting/auditing is trivial
and will naturally become a part of your query practices.  Scan, query, or
stream through your data - Datomic supports all as first-class operations.
The database is an immutable value, like 42 is a value, and a hash-map is a
value - feel ok with holding onto the DB value and using it for a handful of
different operations.  Connections to the DB get cached, don't bother trying
to out-smart the connection mechanism.

All-in-all, Datomic offers you the best tool for dealing with data.  Use it
all to your advantage, however it makes the most sense for your particular
application.

#### Be accurate with your values

Use Datomic's rich schema capabilities to your advantage.  Where possible, don't
store string representations of value-oriented data (like hash maps).  Instead,
break those apart into sets of related attributes and use a ref.

The exception to this rule is if you:

 * Just need to get something into Datomic, quick and dirty
 * The data isn't needed for query and is only ever shipped over the wire to a consumer.

#### Give it time and practice

Datomic's mental model, data model, and information model take some time to
set in - stick with it and push forward.  As you try things out you'll easily
find tactics that work and mistakes to avoid.  Datomic is purposely flexible
and agile enough to adapt as you learn and move forward.  Embrace those
characteristics.

#### Everything can have attributes; Put values on anything

Anything in the database can have attributes and values, including attributes.
This let's you model meta attributes (in the same notion a hyper-graph would),
if that's something you need.

You can additionally add additional attributes and values to facts or transactions
within Datomic.  One useful use case is marking that a particular transaction
came from a particular peer, running a given operation, etc.

#### Picking attribute names

Above there's a tip to make the data relational/rectangular, which subtly
implies all your attribute names would be namespaced.

That's simply not true.

Namespacing an attribute name is only a human-created convention, and sometimes
that convention just doesn't fit your domain.  For example, having an
`:email` attribute might be more reasonable if both "customers" and "sellers"
have emails (rather than making a `:customer/email` and `:seller/email`).

Use attributes that correctly model your domain.  Shoot for the simplest,
smallest building blocks.


## Writing a schema

The extensible hinter produces a schema representation that is very close to
CR-ocs' `short-schema-tx`.

1. Copy and paste the the hinted schema into your service descriptor.
2. Update the documentation strings
3. Adjust the data types of the attributes to your needs (ie: some strings should be longs; some fields are instants)
4. Extend the coercion method to handle any new data conversions that you introduced in the last step
  * Often times, this is not needed, especially if you didn't change the hinted schema

From here, you should be able to do an import without any problems.

## Ingest data


## Where to go from here?

### Building your first API

