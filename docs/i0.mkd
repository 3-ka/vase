Project Overview: cr-ocs
=========================

Consumer Reports On-demand Container Service

This was created in parallel with the original [I0 whiteboard doc](https://docs.google.com/a/thinkrelevance.com/document/d/17VselJYmtjQJNvb5YXXUbsanX0DC-AbrcmJdQ8cy1aE/)
Please see the document for the raw I0 notes, some information only exists there.

Stand-ups are at 9:30am, on Skype.  Call is initiated by a Cognitect employee.

Team members
------------

### Consumer Reports
 * **[David Roubini](mailto://droubini@consumer.org)** - Technology Project Sponsor *(914) 378-2182 // droubini @Skype*
   * What do they do?
 * **[Christian Evans](mailto://cevans@consumer.org)** -  Technical Project Manager *(914) 378-2278 // christianevansnyc @Skype*
   * What do they do?
 * **[Jonah Benton](mailto://jbenton@consumer.org)** - Technical Lead / Architect (Product/story owner) *(555) 555-5555 // jonahrbenton @Skype*
   * What do they do?
 * **[Gui Weinmann](mailto://gweinmann@consumer.org)** - Lead Engineer  *(914) 378-2404 // gui.weinmann @Skype*
   * What do they do?
 * **[Lisa Koenigsburg](mailto://lkoenigsberg@consumer.org)** - Product Manager *(555) 555-5555 // lisa_koenigsberg @Skype*
   * What do they do?
 * **[Matt Mahoney](mailto://mmahoney@consumer.org)** - Mobile Product Manager / Lead *(555) 555-5555 // SkypeName @Skype*
   * What do they do?
 * **[Constantinos (Gus) Mavromoustakos](mailto://cmavromoustakos@consumer.org)** - Mobile Product Manager / Lead *(555) 555-5555 // SkypeName @Skype*
   * What do they do?

### Cognitect
 * **[Naoko Higashide](mailto://naoko@cognitect.com)** - Coach (part-time) *(919) 417-3145 // naoko.hig @Skype*
   * All things project and accounts management
 * **[Paul deGrandis](mailto://paul@cognitect.com)** - Developer, Tech Lead *(603) 401-2102 // ohpauleez @Skype*
   * What do they do?
 * **[Michael Fogus](mailto://fogus@cognitect.com)** - Developer *(555) 555-5555 // fogus.me @Skype*
   * What do they do?


Background
-----------

### Why was this project proposed?

 * CR has recently been undertaking a new Consumer Reports Online Rebuild (CRO-Rebuild)
 * CR needs to adapt how it reaches, engages, converts, and retains its users/subscribers
 * CR is evolving, adapting, and reorganizing its organizational structure, grouped around Consumer Focus Areas (CFA)
   Electronics, Cars, Appliances, etc.
 * This effort goes beyond just the web stack - it pulls in mobile, and even how they manage and organize their data and platforms.
 * We're trying to identify the entire platform to build a consumer/customer-focused organization, product, and process.

 * To illustrate:
   * We'd like to unify the user profile and share data across multiple screens and multiple experiences
   * This data can take any shape (saved searches, saved items, wishlists)
   * Teams need to be able to evolve the consumer focus area specific pieces of data, separately (data versioning and partitioning)


### What business need/challenge are we solving?

 * CFA/Channel developers need a way to rapidly prototype concepts, and gather user feedback
 * The current constraints around data prevent innovation (multiple stores, dependency on reporting/querying, fear to store because of a messy PCI line)
 * CR needs a new innovative digital presence to capture, engage, and retain a newer (younger) consumer base
 * A technology stack that enables (or encourages) a product/consumer-focused workflow
 * Pursuing personalization enhancements as part of the CRO rebuild effort
 * The experience is disconnected, broken, and inconsistent across screens (desktop, mobile, tablet, magazine, etc)


### What are the pain points today?

 * Data is not persisted across multiple devices (or even experiences)
 * The CFA teams need to often bootstrap backend (and establish data setup) because they quickly iterate on product concepts,
   but bootstrapping is costly in terms of time/effort and is not consistent
 * The CFA teams need to rapidly iterate because they need to validate product visions with consumer feedback,
   but currently lacks the organizational and technological infrastructure to work that way
 * Teams are currently siloed in terms of tech stack (responsibilities???), not in a product-focused.
 * Understanding a concepts success relies upon reports from other teams to generate/investigate/analyze data
 * Managing PII (credit cards and privacy) is becoming difficult and highly involved processes


### What auxiliary goals/opportunities are we trying to accomplish or achieve?

 * Versioning APIs and Versioning Schemas within the data
 * Each CFA team has access to its own data, to analyze, query, and investigate
   CFA teams should be able to determine a concepts success without relying on external support
 * CFA teams shouldn't have to deal with paying attention to platform minutia
   * Security best practices should be built in
   * Developers/Consumers should be isolated from upstream/dependency-related patches/versions
 * Solutions should ideally be oriented around data
   * data > functions > macros
 * Demonstrate to the CR organization how the Cognitect process works, and how we use our tools


### What constraints currently exist for the process/system/organization?

 * Constraints are largely legacy, but proactive to adapt, change, and move forward
 * Data access and reporting is a significant issue.
   * A part of that has to do with cross datastore aggregation.  Unable to ask a new question (only reporting on pre-determined/canned query)
 * Data access also comes down to PCI compliance (no way to access anonymous data)
 * Mobile stack and cross department
 * Legacy data store requires schema/model changes to be made in multiple places, and is constrained by PCI scope
 * PCI is not well contained and isolated from everyday development efforts.
 * APIs (as they exist today) are Read-Only


### How long is the (POC) solution expected to last?

 * 0 days (immediate production-conversion upon delivery) to 3 months (but difficult to nail down)

The final product that emerges from the POC will become part of the technological foundation as the company moves forward (5 years)


### *How might we...?* and co-design

 * TODO; See raw I0 notes for brainstorming pieces
  * Code generating template (local development)
  * Hosted "Service Container" that allows the created of new services to be pushed (on a new app root)


Project statement
------------------

```
The (name of team) will
(build, develop, design, implement, etc.)
a (what) for (whom)
```

As of Feb 05, 2014
**The `User Profile Team` will provide templates/scaffolding and tools/software-infrastructure
for Datomic-based User Profile cross channel HTTP-services for Product teams**

As of Feb 17, 2014
**The `CR-ocs Team` will provide a prototype of templates/scaffolding and tools/software-infrastructure
for Datomic-based, data-described, cross channel HTTP-services for Product teams**

### Completion

This project will be be considered **successfully** completed when:

 * CFA/Channel team can use the templates and tools; they are directly consumable and usable.
 * Documentation for the templates and tools; API docs, guides, potentially tutorials for developers and operations.
 * CFA/Channel team can query/analyze/aggregate data related to a concept, without relying on an external team/entity/process
   * *Only when the concept is built only with the templates/tools.*
 * Deployed on EC2 and Dynamo (but should work locally)

This has been updated as of **Feb 17, 2014**

 * The templates/tools are functional
 * Basic documentation for developers are delivered
 * Product Owner as CFA/Channel developer proxy can query/analyze/aggregate data related to a concept/prototype, within templates and tools
 * The container service can be deployed locally or on EC2, using Datomic Free or Pro Starter license


### Win conditions

Everyone gets to pick three

| Condition             |   1   |   2   |   3   | Definition                   |
| :-------------------- | :---: | :---: | :---: | :--------------------------- |
| Schedule              |       |       |       |                              |
| Scope                 |       |       |   X   | Tight scope; Tightly managed scope-creep; Set right expectations. |
| Quality               |       |       |       |                              |
| Budget                |       |       |       |                              |
| Customer satisfaction |       |   X   |       | Developers (Gus, Gui) Building relationships with digital product team (Lisa, Matt) |
| Teamwork/Learning     |   X   |       |       | Project team.  Validate CR Cognitect teamwork, work style, partnership. |

 1. Teamwork / Learning
 2. Customer satisfaction
 3. Scope


### Scope

| In Scope                       | Out of Scope                             |
| :----------------------------: | :--------------------------------------: |
| Development & Testing          | Cross-browser testing                    |
| Documentation                  | Scala education                          |
|                                | UI design work by Relevance              |
|                                | Production deployment                    |
|                                | Load testing *                           |
|                                | Performance testing *                    |
|                                | Post-deployment support                  |
|                                | Security audit *                         |
|                                | Application hosting                      |

#### Scope statement

We will deliver a set of tools and templates for CR Devs to use to build
RESTful services.  The tools will embrace security best practices, such as CSRF and XSS protection.
The tools will also isolate developers from upstream/dependency related patches, by versioning
the tool/template/platform itself.  The services will be responsible for validating incoming data and
providing read/write access to the data of which the service is the canonical representation.
The tools and templates should always allow developers to access the underlying technologies directly -
for example, direct access to Datomic.  Bootstrapping the platform should be low effort barrier -
ideally a single/few step operation.

**Scope Update: Feb 17, 2014**

The scope has been reduced to remove the data/input validation, security
protections, robust (simulation) testing, and usability enhancements.  The
remaining system will only: Transact data, query data, return static responses,
and redirect to other URLs.  Data/input will not be sanitized and there are
no security measures to guard against malicious requests.

- - - -

The tool should:

 * Favor data, over functions, over macros
 * Configure endpoints, schema, and potentially even validations as data
 * Provide a mechanism for versioning the service and data/schema (semantic and git hash would be ideal)
 * Always be consumable as an HTTP Rest interface (if not consumable as a language library)

We will design and implement a standard JSON message format to which all developed services will
conform.

Future iterations after the initial POC may tackle:

 * Adding in reduced-scope features: security concerns/protection, input validation, usability enhancements, robust testing
 * Providing a compatibility layer with CR's current API verbs/layout
 * Multilanguage support (direct consumption and use within alternate languages - lowest barrier for those prototyping).
 * Operating behind and integrating with Mashery
 * Large text objects, operations, & Lucene indexing
 * Services supporting Websockets / steaming (soft real time) / Server sent events


### Quality attributes and scenarios

Everyone gets to pick three
Definitions from [MSDN Architecture Guide](http://msdn.microsoft.com/en-us/library/ee658094.aspx)

| Attribute       |   1   |   2   |   3   | Definition                              |
| :-------------- | :---: | :---: | :---: | :-------------------------------------- |
| Scalability     |       |       |       | The ability of a system to handle increases in load without impact on the performance/runtime characteristics of the system |
| Security        |       |   X   |       | The capability of a system to prevent malicious or accidental actions outside of the designed usage, and to prevent disclosure or loss of information |
| Modifiability   |   X   |       |       | The ability of the system to undergo changes with a degree of ease |
| Maintainability |       |       |       | The ease of which the system is monitored, serviced, deployed, updated; Manageability |
| Reliability     |       |       |       | The ability of a system to remain operational over time, in the face of errors/defects/unknowns |
| Availability    |       |       |       | The proportion of time that the system is functional and working |
| Robustness      |       |       |       | The extent to which a software system can tolerate changes in its environment without intervention |
| Adaptability    |       |       |       | The extent to which a software system adapts to change in its environment without intervention |
| Usability       |       |       |   X   | Conformance to requirements, fitting [good design criteria](https://en.wikipedia.org/wiki/Heuristic_evaluation#Nielsen.27s_heuristics) |
| Performance     |       |       |       | Do you mean latency? throughput? etc? |

#### Quality attribute ranking

Modifiability > Security > Usability

#### Scenarios // thresholds

 * This is a tool for mocking, evolving, and understanding data shapes.  Common data operations (schema changes, potentially even validations) should be data-only modifications.
 * Single source of truth for each individual piece of application/configuration data (loosely coupled, highly cohesive)
 * Additional endpoint operations require minimal data configuration, and BYOF (bring your own function)
 * The system itself is tolerant of post-prototype rapid iterations (there is no barrier with regards to data/schema evolution)
 * *Punted* - Sanitization hooks are in place, but not forced upon users.  Potentially there’s a default sanitize function.
 * *Punted* - All common attack vectors are blocked: XSS, CSRF, Injection
 * *Punted* - Security components are integrated ideally at the Pedestal Interceptor level
 * Incoming requests are trusted through an external mechanism (API Keys/Trusted marker), but that is not the responsibility of this tool/generated service
 * Use Nielsen’s Heuristics as the usability checklist


### Known risks and areas of unknown risks

 * Organization is in flux - CEO search is still active
 * Unclear how mission-driven workflow will be adapted to be consumer focused when delivering new products
 * Still unclear how excited the entire organization is about this tech/company/process evolution.  Potentially this project is being used to get the ball rolling
 * Product-driven priorities are unclear and how they’ll shape the process and organization (and how they might cause friction)
 * Unclear how multiple data stores (in and out of the data center) of user data can (if at all) be linked, combined, united.  (For example, survey data)


Metrics
-------

Metrics help us understand the project's context and progress.  They aid in
recognizing and identifying when project-wide problems occur.  Metrics provide
a baseline of objective data for making informed project-oriented decisions.

### Project metrics

 * Iteration burn up - points dev-completed within an iteration
 * Project burn down - total points remaining for the project (delivered to production)
 * Running tested features - cards or points delivered to production (not including defects or tasks)

### Software metrics


### System metrics


Extra
-----

### Special Conditions

 * Open source?
 * IP Restrictions?
 * Special documents?
 * Special clearance?
 * Additional technology restrictions?


### Baseline top-down estimations

 * Use [COCOMO with Monte Carlo simulation](http://csse.usc.edu/tools/COCOMO.php) to get an idea of where the project might end up
  * [Our project](./ext/cr-cocomo-estimation.pdf) is estimated at 3-4 person-months of effort.


### Baseline bottom-up estimations

Break down all major milestones into their individual tasks, and estimate effort (person-months) based on historical data

 * ~30-40 Person-days of effort (1-2.5 person-months)
 * 45 story points of effort


Schedule
--------
*Caveat: if the per-phase assumptions are false, phases will need to be adjusted/adapted*

### Phase 1 :: 6-8 person days :: 10 SLOC :: 11 points

 * Writing a cards and producing Top-down/bottom up estimate
 * Land I0, Design, Education, Quicklearn docs
 * Reaching out and confirming with immediate users (Gui, Gus) the ideal mode of interaction.
   * HTTP (they'll send payload of EDN)
   * Templates that generate a "best practices" platform (also used to build SOSP)
 * Design, define, and confirm around route formats; then document
 * Design, define, and confirm schema formats; then document
 * Design, define, and confirm validation formats; then document
 * Design, define, and confirm application configuration formats; then document
 * Design, define, and confirm the message/payload format for talking to the SOSP; then document
 * Integrate testing harness; Generative/property tests executed by default runner


### Phase 2 :: 8-12 person-days :: 120 SLOC :: 22 points

 * Bootstrap tooling to accept config/format conventions
   * Pedestal routes are resolved as added
   * Validation functions resolve
 * (lein and shell) Templates to generate the base SOSP system
 * Pedestal Interceptors added for security capabilities (if not present)
 * Interceptor for server time
 * Interceptor for request tagging/token for tracing (via header inspection)
 * SOSP provides working services at app-root upon submission, but no update (only overwrite)
 * Client-side file/script to make use and integration testing easier
 * Integrate simulation testing harness


### Phase 3 :: 6-10 person-days :: 30 SLOC :: 12 points

 * Simulation testing scenarios
   * Creation-only (with overwrite)
   * Creation and update
 * SOSP allows app-roots to be updated (and relaunched) with new data/config definitions
 * Update client-side script to do `update` functionality
 * A dir watcher on the developer machine streams updates to SOSP
 * Document complete workflow


### Phase 4 :: 6-10 person-days (no bonus) :: 20 SLOC

 * Final presentation/call/walkthrough
 * Bug fixes and polish
 * Total bonus points:
   * Mobile web/app that interacts with a SOSP’d service
   * Video/screencast walkthrough

